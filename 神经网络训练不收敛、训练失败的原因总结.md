### 神经网络训练不收敛、训练失败的原因总结



下面从数据和模型两个方面去分析模型不收敛或者失败的原因。

#### 一、数据与标签

1. 没有对数据进行**预处理**。数据分类标注是否正确？数据是否干净？

2. 没有对数据进行**归一化**。由于不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行标准化处理，以解决指标之间的可比性。原始数据通过标准化后，各指标会处于同一数量级，适合进行综合对比评价。<u>涉及权值初始化、激活函数、优化算法等。</u>

   **方式：（数据 - 均值）/ 方差**

3. 样本的信息量太大导致网络不足以`fit`住整个样本空间。

   1. `training set`和`validate set`差别过大，就没办法训练出合适的效果；
   2. 样本少，带来过拟合的问题，这时候可能就需要`dropout`,`SGD`,增大`minibatch`的数量，减少`fc`层的节点数量，`momentum`,`finetune`（微调）等；

4. 标签的设置是否正确。



#### 二、模型方面

**1、网络设定不合理**

如果做很复杂的分类任务，却只用了很浅的网络，可能会导致训练难以收敛。应当选择合适的网络，或者尝试加深当前网络。总体来说，网络不是越深越好，开始可以搭建一个3~8层的网络，当这个网络实现的不错时，你可以考虑实验更深的网络来提升精确度。从小网络开始训练意味着更快，并且可以设置不同参数观察对网络的影响而不是简单的堆叠更多层。 

**2、Learning rate不合适**

如果太大，会造成无法收敛；如果太小，会造成收敛速度非常慢。

在自己训练新网络时，可以从0.1开始尝试，如果loss不下降的意思，那就降低，除以10，用0.01尝试，一般来说0.01会收敛，不行的话就用0.001. 学习率设置过大，很容易震荡。不过刚刚开始不建议把学习率设置过小，尤其是在训练的开始阶段。在开始阶段我们不能把学习率设置的太低否则loss不会收敛。

我的做法是逐渐尝试，从0.1,0.08,0.06,0.05 ......逐渐减小直到正常为止。有的时候候学习率太低走不出低估，把冲量提高也是一种方法，适当提高mini-batch值，使其波动不大。

learning rate设大了会带来跑飞（loss突然一直很大）的问题。这个是新手最常见的情况——为啥网络跑着跑着看着要收敛了结果突然飞了呢？<u>可能性最大的原因是你用了relu作为激活函数的同时使用了softmax或者带有exp的函数做分类层的loss函数。</u>

当某一次训练传到最后一层的时候，某一节点激活过度（比如100），那么exp(100)=Inf，发生溢出，bp后所有的weight会变成NAN，然后从此之后weight就会一直保持NAN，于是loss就飞起来辣。如果lr设的过大会出现跑飞再也回不来的情况。这时候你停一下随便挑一个层的weights看一看，很有可能都是NAN了。对于这种情况建议用二分法尝试。0.1~0.0001.不同模型不同任务最优的lr都不一样。

**3、隐藏神经元数量错误**

神经元数量太少：没有能力来表达任务；

神经元数量太多：会导致训练缓慢，并且网络很难清除一些噪声。

这个设置可以参考研究人员使用的数字，往往具备参考价值；可以先设置小的，然后再慢慢增加这个数字。

<u>如果是回归任务，可以考虑使用神经元数量为输入或输出变量的2到3倍。</u>

在实际中，隐藏单元的数量，相对比其他影响因素来说，对神经网络的性能影响相当小。更多地，增大所需要隐藏单元的数量仅仅是减缓了训练速度。

**4、错误初始化网络参数**

没有正确的初始化网络权重，使得网络没办法进行训练。

**5、没有正则化**

正则化典型的就是dropout、加噪声等。即使数据量很大或者你觉得网络不可能出现过拟合，但是对网络进行正则化还是很有必要的。 

dropout 通常从设定参数为0.75或0.9开始，根据你认为网络出现过拟合的可能性来调整这个参数。另外，如果你确定这个网络不会出现过拟合，那么可以将参数设定为0.99。正则化不仅仅可以防止过拟合，并且在这个随机过程中，能够加快训练速度以及帮助处理数据中的异常值并防止网络的极端权重配置。**对数据扩增也能够实现正则化的效果，最好的避免过拟合的方法就是有大量的训练数据**。 

**6、Batch size 过大**

Batch size 设置的过大会降低网络的准确度，因为它降低了梯度下降的随机性。另外，在相同情况下batch size 越大那么要达到相同的精确度通常需要训练更多的epoch。 

我们可以尝试一些较小的batch size 如 16 ，8 甚至是1。使用较小的batch size 那么一个epoch就可以进行更多次的权值更新。这里有两个好处，第一，可以跳出局部最小点。其二可以表现出更好的泛化性能。 （对每个样本都能进行一个良好的学习）

**7、最后一层的激活函数用得不对**

在最后一层使用错误的激活函数会导致网络最终不能输出你期望的范围值，最常见的错误就是最后一层使用Relu函数，其输出无负值。 

如果是做**回归任务**，大多数情况下不需要使用激活函数，除非你知道你所期望的值作为输出。想象一下你的数据值实际代表了什么，以及再归一化之后它们的范围是多少，<u>最有可能的情况是输出没有边界的正数和负数。在这种情况下，最后一层不应该使用激活函数</u>。<u>如果你的输出值只能在某个范围内有意义，如0~1范围内的概率组成。那么最后一层可以使用sigmoid函数</u>。 

**8、网络存在坏梯度**

如果你训练了几个epoch误差没有改变,那可能是你使用了**Relu**，可以尝试将激活函数换成**leaky Relu**。因为Relu激活函数对正值的梯度为1，负值的梯度为0。因此会出现某些网络权值的成本函数的斜率为0，在这种情况下我们说网络是“dead”,因为网络已经不能更新。 



**通过 train loss 和 test loss 分析网络当下的状况：**

1. train loss 不断下降，test loss不断下降，说明网络仍在学习;
2. train loss 不断下降，test loss趋于不变，说明网络过拟合;
3. train loss 趋于不变，test loss不断下降，说明数据集100%有问题;
4. train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;
5. train loss 不断上升，test loss不断上升，说明网络结构设计不当，训练超参数设置不当，数据集需要经过清洗等问题。